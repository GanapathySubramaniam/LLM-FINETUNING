{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”§ Inputs Required for Finetuning\n",
    "\n",
    "The following inputs are essential for the finetuning process:\n",
    "\n",
    "### 1. Dataset\n",
    "- High-quality training data\n",
    "- Input/output pairs for supervised learning\n",
    "\n",
    "### 2. Model and Training Arguments\n",
    "- Pre-trained model selection\n",
    "- Learning parameters configuration\n",
    "- Training environment setup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"What are the benefits of renewable energy?\",\n",
    "    \"Describe the process of photosynthesis.\",\n",
    "    \"What is the significance of the Renaissance?\",\n",
    "    \"How does the human immune system work?\"\n",
    "]\n",
    "responses = [\n",
    "    \"Quantum computing uses quantum bits, or qubits, to perform calculations. Unlike classical bits that are either 0 or 1, qubits can exist in multiple states simultaneously, allowing quantum computers to solve certain complex problems faster.\",\n",
    "    \"Renewable energy, such as solar and wind, reduces greenhouse gas emissions, decreases air pollution, and conserves natural resources. It also promotes energy independence and sustainability.\",\n",
    "    \"Photosynthesis is the process by which green plants use sunlight to make food from carbon dioxide and water. It occurs in the chloroplasts, producing oxygen as a byproduct.\",\n",
    "    \"The Renaissance was a cultural movement from the 14th to the 17th century, characterized by a renewed interest in classical art, science, and philosophy. It led to significant advancements in many fields and a shift towards humanism.\",\n",
    "    \"The human immune system protects the body from infections and diseases. It consists of physical barriers, immune cells, and proteins that identify and destroy pathogens like bacteria and viruses.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='meta-llama/Llama-3.2-1B'\n",
    "output_path='./finetuned_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args={\n",
    "                \"overwrite_output_dir\": True,# ğŸ”„ Whether to overwrite existing output directory\n",
    "                \"eval_strategy\": \"no\",# ğŸ“Š Evaluation strategy during training\n",
    "                \"learning_rate\": 2e-5, # ğŸ“ˆ Learning rate for model optimization\n",
    "                \"per_device_train_batch_size\": 1, # ğŸ“¦ Number of samples processed per device per training step\n",
    "                \"gradient_accumulation_steps\": 4,# ğŸ”„ Number of steps to accumulate gradients before updating weights\n",
    "                \"num_train_epochs\": 3,# ğŸ” Number of complete passes through training dataset\n",
    "                \"weight_decay\": 0.01, # âš–ï¸ L2 regularization factor to prevent overfitting\n",
    "                \"fp16\": True, # ğŸš€ Enable mixed precision training for faster computation\n",
    "                \"gradient_checkpointing\": True# ğŸ’¾ Enable gradient checkpointing to save memory\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM,Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from huggingface_hub import login\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¦ Custom Dataset Class\n",
    "- ğŸ”§ A predefined class that creates custom PyTorch datasets for model finetuning\n",
    "- ğŸ”„ Handles data loading and batch preparation during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.inputs[\"attention_mask\"][idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”§ Finetuner Class\n",
    "- ğŸš€ Handles the complete finetuning pipeline\n",
    "- ğŸ¤– Manages model and tokenizer initialization\n",
    "- ğŸ“Š Processes and prepares training data\n",
    "- âš™ï¸ Configures training parameters\n",
    "- ğŸ“ˆ Executes model training and saves results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class finetuner:\n",
    "  def __init__(self, model_name):\n",
    "    login(token='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')\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "  def enable_gradient_checkpointing(self):\n",
    "    self.model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "\n",
    "  def pad_tokenizer(self):\n",
    "    if self.tokenizer.pad_token is None:\n",
    "      self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "  def tokenize_data(self):\n",
    "    max_length = 50\n",
    "    # Tokenize prompts and responses\n",
    "    self.tokenized_inputs = self.tokenizer(self.inputs, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    self.tokenized_labels = self.tokenizer(self.outputs, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    # Ensure labels' padding tokens are ignored in loss computation\n",
    "    self.tokenized_labels[self.tokenized_labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "  def create_dataset(self,indices):\n",
    "     inputs={\n",
    "                'input_ids':self.tokenized_inputs[\"input_ids\"][indices],\n",
    "                'attention_mask':self.tokenized_inputs[\"attention_mask\"][indices]\n",
    "            }\n",
    "     labels=self.tokenized_labels[indices]\n",
    "     return CustomDataset(inputs,labels)\n",
    "    \n",
    "  def split_dataset(self):\n",
    "    indices = list(range(len(self.tokenized_inputs[\"input_ids\"])))\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=self.test_size, random_state=self.random_seed)\n",
    "    self.train_dataset = self.create_dataset(train_indices)\n",
    "    self.eval_dataset = self.create_dataset(val_indices)\n",
    "\n",
    "  def prepare_dataset(self,inputs,outputs):\n",
    "    self.inputs=inputs\n",
    "    self.outputs=outputs\n",
    "  \n",
    "  def collate_data(self):\n",
    "    self.data_collator = DataCollatorForSeq2Seq(\n",
    "      tokenizer=self.tokenizer,\n",
    "      model=self.model,\n",
    "      padding=True)\n",
    "  \n",
    "  def prepare_training_Args(self,output_path):\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    self.training_args = TrainingArguments(output_dir=output_path,**training_args)\n",
    "    self.trainer = Trainer( \n",
    "                            model=self.model,\n",
    "                            args=self.training_args,\n",
    "                            train_dataset=self.train_dataset,\n",
    "                            eval_dataset=self.eval_dataset,\n",
    "                            data_collator=self.data_collator)\n",
    "    \n",
    "  def train(self):\n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        self.trainer.train()\n",
    "    except ValueError as e:\n",
    "        print(\"\\nError during training:\")\n",
    "        print(e)\n",
    "\n",
    "  def save_model(self):\n",
    "    self.model.save_pretrained(self.output_path)\n",
    "    self.tokenizer.save_pretrained(self.output_path)\n",
    "\n",
    "  def run(self,inputs,outputs,output_path,train_size=0.8,random_seed=42):\n",
    "    self.random_seed=42\n",
    "    self.test_size=1-train_size\n",
    "    self.output_path=output_path\n",
    "    self.enable_gradient_checkpointing()\n",
    "    self.pad_tokenizer()\t\n",
    "    self.prepare_dataset(inputs,outputs)\n",
    "    self.tokenize_data()\n",
    "    self.prepared_dataset = CustomDataset(self.tokenized_inputs, self.tokenized_labels)\n",
    "    self.collate_data()\n",
    "    self.split_dataset()\n",
    "    self.prepare_training_Args(output_path)\n",
    "    self.train()\n",
    "    self.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”§ Fine-tuning the Model\n",
    "\n",
    "This section handles the model fine-tuning process using custom training data. The fine-tuning will adapt the base model to better handle our specific use case.\n",
    "\n",
    "Key steps:\n",
    "- Initialize the fine-tuning process\n",
    "- Train on custom dataset\n",
    "- Save the fine-tuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuner_instance = finetuner(model_name)\n",
    "finetuner_instance.run(prompts,responses,output_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
