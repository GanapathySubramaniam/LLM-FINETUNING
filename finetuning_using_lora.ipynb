{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCYRLmcezF3q"
      },
      "source": [
        "# Finetuning a LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1ksUQGXzF3r"
      },
      "source": [
        "### üîß Inputs Required for Finetuning\n",
        "\n",
        "The following inputs are essential for the finetuning process:\n",
        "\n",
        "### 1. Dataset\n",
        "- High-quality training data\n",
        "- Input/output pairs for supervised learning\n",
        "\n",
        "### 2. Model and Training Arguments\n",
        "- Pre-trained model selection\n",
        "- Learning parameters configuration\n",
        "- Training environment setup\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQZLRKmezF3r"
      },
      "source": [
        "## 1. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JYF6oNYfzF3s"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Explain quantum computing in simple terms.\",\n",
        "    \"What are the benefits of renewable energy?\",\n",
        "    \"Describe the process of photosynthesis.\",\n",
        "    \"What is the significance of the Renaissance?\",\n",
        "    \"How does the human immune system work?\"\n",
        "]\n",
        "responses = [\n",
        "    \"Quantum computing uses quantum bits, or qubits, to perform calculations. Unlike classical bits that are either 0 or 1, qubits can exist in multiple states simultaneously, allowing quantum computers to solve certain complex problems faster.\",\n",
        "    \"Renewable energy, such as solar and wind, reduces greenhouse gas emissions, decreases air pollution, and conserves natural resources. It also promotes energy independence and sustainability.\",\n",
        "    \"Photosynthesis is the process by which green plants use sunlight to make food from carbon dioxide and water. It occurs in the chloroplasts, producing oxygen as a byproduct.\",\n",
        "    \"The Renaissance was a cultural movement from the 14th to the 17th century, characterized by a renewed interest in classical art, science, and philosophy. It led to significant advancements in many fields and a shift towards humanism.\",\n",
        "    \"The human immune system protects the body from infections and diseases. It consists of physical barriers, immune cells, and proteins that identify and destroy pathogens like bacteria and viruses.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqDpRqktzF3s"
      },
      "source": [
        "## 2. Model and Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "18juc4gezF3s"
      },
      "outputs": [],
      "source": [
        "model_name='meta-llama/Llama-3.2-1B'\n",
        "output_path='./finetuned_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o2OFAP0DzF3t"
      },
      "outputs": [],
      "source": [
        "training_args={\n",
        "                \"overwrite_output_dir\": True,# üîÑ Whether to overwrite existing output directory\n",
        "                \"eval_strategy\": \"no\",# üìä Evaluation strategy during training\n",
        "                \"learning_rate\": 2e-5, # üìà Learning rate for model optimization\n",
        "                \"per_device_train_batch_size\": 1, # üì¶ Number of samples processed per device per training step\n",
        "                \"gradient_accumulation_steps\": 4,# üîÑ Number of steps to accumulate gradients before updating weights\n",
        "                \"num_train_epochs\": 3,# üîÅ Number of complete passes through training dataset\n",
        "                \"weight_decay\": 0.01, # ‚öñÔ∏è L2 regularization factor to prevent overfitting\n",
        "                \"fp16\": True, # üöÄ Enable mixed precision training for faster computation\n",
        "                \"gradient_checkpointing\": True,# üíæ Enable gradient checkpointing to save memory\n",
        "                \"auto_find_batch_size\":True,\n",
        "                }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58ker2VAzF3t"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GS3jQqdTzM8y"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft==0.8.2\n",
        "!pip install -q datasets==2.16.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3HiD9_nzzoNt"
      },
      "outputs": [],
      "source": [
        "# TARGET_MODULES\n",
        "# https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Cy-gevnLzF3t"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM,Trainer, TrainingArguments,DataCollatorForLanguageModeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "import peft\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHDAIOL6zF3u"
      },
      "source": [
        "### üì¶ Custom Dataset Class\n",
        "- üîß A predefined class that creates custom PyTorch datasets for model finetuning\n",
        "- üîÑ Handles data loading and batch preparation during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4BX4sbFlzF3u"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.inputs[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.inputs[\"attention_mask\"][idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4-HJBaczF3u"
      },
      "source": [
        "### üîß Finetuner Class\n",
        "- üöÄ Handles the complete finetuning pipeline\n",
        "- ü§ñ Manages model and tokenizer initialization\n",
        "- üìä Processes and prepares training data\n",
        "- ‚öôÔ∏è Configures training parameters\n",
        "- üìà Executes model training and saves results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2gmhWNZszF3u"
      },
      "outputs": [],
      "source": [
        "\n",
        "class finetuner:\n",
        "    def __init__(self, model_name):\n",
        "        # login(token='your_token_here')  # Make sure to replace with your actual token\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    def configure_lora(self):\n",
        "        self.lora_config = LoraConfig(\n",
        "            r=4,\n",
        "            lora_alpha=1,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.05,\n",
        "            bias=\"lora_only\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "    def get_peft_model(self):\n",
        "        self.peft_model = get_peft_model(self.model, self.lora_config)\n",
        "        # Ensure LoRA parameters require gradients\n",
        "        for param in self.peft_model.parameters():\n",
        "            if param.requires_grad:\n",
        "                break\n",
        "        else:\n",
        "            print(\"No parameters require gradients!\")\n",
        "\n",
        "    def enable_gradient_checkpointing(self):\n",
        "        self.model.gradient_checkpointing_enable()\n",
        "        self.model.config.use_cache = False  # Disable use_cache when using gradient checkpointing\n",
        "\n",
        "    def pad_tokenizer(self):\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def tokenize_data(self):\n",
        "        max_length = 50\n",
        "        # Tokenize prompts and responses\n",
        "        self.tokenizer_config={'padding':'max_length','truncation':True,'max_length':max_length,'return_tensors':'pt'}\n",
        "        self.tokenized_inputs = self.tokenizer(self.inputs, **self.tokenizer_config)\n",
        "        self.tokenized_labels = self.tokenizer(self.outputs, **self.tokenizer_config)[\"input_ids\"]\n",
        "        # Ensure labels' padding tokens are ignored in loss computation\n",
        "        self.tokenized_labels[self.tokenized_labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    def create_dataset(self, indices):\n",
        "        inputs = {\n",
        "            'input_ids': self.tokenized_inputs[\"input_ids\"][indices],\n",
        "            'attention_mask': self.tokenized_inputs[\"attention_mask\"][indices]\n",
        "        }\n",
        "        labels = self.tokenized_labels[indices]\n",
        "        return CustomDataset(inputs, labels)\n",
        "\n",
        "    def split_dataset(self):\n",
        "        indices = list(range(len(self.tokenized_inputs[\"input_ids\"])))\n",
        "        train_indices, val_indices = train_test_split(indices, test_size=self.test_size, random_state=self.random_seed)\n",
        "        self.train_dataset = self.create_dataset(train_indices)\n",
        "        self.eval_dataset = self.create_dataset(val_indices)\n",
        "\n",
        "    def prepare_dataset(self, inputs, outputs):\n",
        "        self.inputs = inputs\n",
        "        self.outputs = outputs\n",
        "\n",
        "    def collate_data(self):\n",
        "        self.data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,\n",
        "        )\n",
        "\n",
        "    def prepare_training_Args(self, output_path, training_args):\n",
        "        training_args['output_dir'] = output_path\n",
        "        os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "        self.training_args = TrainingArguments(**training_args)\n",
        "        self.trainer = Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=self.training_args,\n",
        "            train_dataset=self.train_dataset,\n",
        "            eval_dataset=self.eval_dataset,\n",
        "            data_collator=self.data_collator\n",
        "        )\n",
        "\n",
        "    def train(self):\n",
        "        torch.cuda.empty_cache()\n",
        "        try:\n",
        "            self.trainer.train()\n",
        "        except ValueError as e:\n",
        "            print(\"\\nError during training:\")\n",
        "            print(e)\n",
        "\n",
        "    def save_model(self):\n",
        "        self.peft_model.save_pretrained(self.output_path)\n",
        "        self.tokenizer.save_pretrained(self.output_path)\n",
        "\n",
        "    def run(self, inputs, outputs, output_path, train_size=0.8, random_seed=42):\n",
        "        self.random_seed = random_seed\n",
        "        self.test_size = 1 - train_size\n",
        "        self.output_path = output_path\n",
        "        self.enable_gradient_checkpointing()\n",
        "        self.pad_tokenizer()\n",
        "        self.configure_lora()\n",
        "        self.get_peft_model()\n",
        "        self.prepare_dataset(inputs, outputs)\n",
        "        self.tokenize_data()\n",
        "        self.collate_data()\n",
        "        self.split_dataset()\n",
        "        self.prepare_training_Args(output_path, training_args)\n",
        "        self.train()\n",
        "        self.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NIpPY1bzF3u"
      },
      "source": [
        "### üîß Fine-tuning the Model\n",
        "\n",
        "This section handles the model fine-tuning process using custom training data. The fine-tuning will adapt the base model to better handle our specific use case.\n",
        "\n",
        "Key steps:\n",
        "- Initialize the fine-tuning process\n",
        "- Train on custom dataset\n",
        "- Save the fine-tuned model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "89C_bVC7zF3u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "126c748c-e822-469f-e760-0ac4e451d536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "finetuner_instance = finetuner(model_name)\n",
        "finetuner_instance.run(prompts,responses,output_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "EG7GsQQF5EPa"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(output_path)"
      ],
      "metadata": {
        "id": "egAzdPwA53t3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(model, output_path)"
      ],
      "metadata": {
        "id": "2EPHL7kA5mVk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqutStqO5ofY",
        "outputId": "2bfe37d0-2ccc-4767-f72d-1baa87c468dc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-15): 16 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "              (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M65CH-oh5qVy",
        "outputId": "5b826fee-7d9c-49ec-870f-da7d3aa893bd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-15): 16 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "              (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_prompt = \"Tell me a joke about cats.\"\n",
        "input_ids = tokenizer.encode(input_prompt, return_tensors='pt').to(device)"
      ],
      "metadata": {
        "id": "7WOoz5is5trx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=100,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx9oqQIT5zPj",
        "outputId": "57eb6201-4a19-4e7d-93df-1bf927d4f2fc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbEbCHyc5-RZ",
        "outputId": "1a11decf-bbc1-472b-b330-e7ab58084a89"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a joke about cats. I‚Äôll give you a hint: it‚Äôs not about the tail.\n",
            "We‚Äôre all aware of the saying ‚Äúcats are known for their tails.‚Äù But what if we told you that the origin of this phrase actually comes from a completely different place?\n",
            "A cat‚Äôs tail is often thought of as a symbol of strength, loyalty, and independence. But in the ancient world, it was believed that cats had tails that were used as weapons. In fact, some ancient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yRJYtyp-6BKY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}